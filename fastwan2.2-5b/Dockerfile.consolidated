# syntax=docker/dockerfile:1.7
###############################################################################
# CONSOLIDATED DOCKERFILE FOR FASTWAN 2.2-5B WITH COMFYUI
# Optimized for RTX 4090 GPU
###############################################################################

###############################################################################
# BUILD STAGE #################################################################
###############################################################################
ARG CUDA_VERSION=12.6.0
ARG UBUNTU_VERSION=22.04
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu${UBUNTU_VERSION} AS builder

# --- Base tooling -----------------------------------------------------------
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python3 python3-venv python3-distutils python3-pip \
        build-essential git curl ca-certificates aria2 && \
    rm -rf /var/lib/apt/lists/*

# --- Python venv & Core ML stack ---------------------------------------------
ARG TORCH_CUDA=cu126
ARG TORCH_VER=2.6.0
ARG TVISION_VER=0.21.0+${TORCH_CUDA}
ARG TAUDIO_VER=2.6.0+${TORCH_CUDA}
ARG XFORMERS_VER=0.0.29.post2

ENV VENV_PATH=/opt/venv
RUN python3 -m venv ${VENV_PATH} && \
    . ${VENV_PATH}/bin/activate && \
    pip install --upgrade --no-cache-dir pip setuptools wheel && \
    pip install --extra-index-url https://download.pytorch.org/whl/${TORCH_CUDA} \
        torch==${TORCH_VER}+${TORCH_CUDA} \
        torchvision==${TVISION_VER} \
        torchaudio==${TAUDIO_VER} \
        xformers==${XFORMERS_VER} \
        onnx==1.16.2 onnxruntime-gpu==1.22.0 numpy==1.26.4 && \
    curl -L -o /tmp/sageattention.whl \
        https://huggingface.co/MonsterMMORPG/SECourses_Premium_Flash_Attention/resolve/main/sageattention-2.1.1-cp310-cp310-linux_x86_64.whl && \
    mv /tmp/sageattention.whl /tmp/sageattention-2.1.1-py3-none-any.whl && \
    pip install /tmp/sageattention-2.1.1-py3-none-any.whl && rm /tmp/sageattention-2.1.1-py3-none-any.whl && \
    find ${VENV_PATH} -name '__pycache__' -type d -exec rm -rf {} + && \
    pip cache purge

# --- Clone ComfyUI and install dependencies ---------------------------------
WORKDIR /workspace
RUN git clone https://github.com/comfyanonymous/ComfyUI.git && \
    cd ComfyUI && \
    . ${VENV_PATH}/bin/activate && \
    pip install --no-cache-dir -r requirements.txt

# --- Download FastWAN 2.2-5B Models ----------------------------------------
WORKDIR /workspace/ComfyUI
RUN mkdir -p models/diffusion_models models/text_encoders models/vae models/loras custom_nodes

# Download FastWAN 2.2-5B models using aria2c for faster downloads
RUN aria2c -x 16 -s 16 --dir="models/diffusion_models" -o wan2.2_ti2v_5B_fp16.safetensors \
    "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors?download=true" && \
    aria2c -x 16 -s 16 --dir="models/text_encoders" -o umt5_xxl_fp8_e4m3fn_scaled.safetensors \
    "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true" && \
    aria2c -x 16 -s 16 --dir="models/vae" -o wan2.2_vae.safetensors \
    "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan2.2_vae.safetensors?download=true" && \
    aria2c -x 16 -s 16 --dir="models/loras" -o Wan2_2_5B_FastWanFullAttn_lora_rank_128_bf16.safetensors \
    "https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/FastWan/Wan2_2_5B_FastWanFullAttn_lora_rank_128_bf16.safetensors?download=true"

# Clone FastWAN custom node
RUN git clone https://github.com/FNGarvin/fastwan-moviegen.git custom_nodes/fastwan-moviegen

# Install custom node dependencies
RUN . ${VENV_PATH}/bin/activate && \
    if [ -f "custom_nodes/fastwan-moviegen/requirements.txt" ]; then \
        cd custom_nodes/fastwan-moviegen && \
        pip install --no-cache-dir -r requirements.txt; \
    fi

###############################################################################
# RUNTIME STAGE ###############################################################
###############################################################################
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu${UBUNTU_VERSION}

# Install runtime system dependencies
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
        python3 python3-dev git curl ca-certificates gpg ffmpeg tini && \
    \
    # Add repositories and install Caddy & Tailscale
    curl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg && \
    curl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | tee /etc/apt/sources.list.d/caddy-stable.list > /dev/null && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.noarmor.gpg | gpg --dearmor -o /usr/share/keyrings/tailscale-archive-keyring.gpg && \
    curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/jammy.tailscale-keyring.list | tee /etc/apt/sources.list.d/tailscale.list > /dev/null && \
    apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends caddy tailscale && \
    rm -rf /var/lib/apt/lists/*

# Copy the pre-built Python virtual environment and ComfyUI with models
ENV VENV_PATH=/opt/venv
COPY --from=builder ${VENV_PATH} ${VENV_PATH}
COPY --from=builder /workspace/ComfyUI /workspace/ComfyUI

# Copy configuration and startup script
WORKDIR /workspace
COPY Caddyfile /etc/caddy/Caddyfile
COPY start_services.sh /usr/local/bin/start_services.sh
COPY workflow_api.json /workspace/workflow_api.json
COPY api_wrapper.py /workspace/api_wrapper.py
COPY api_requirements.txt /workspace/api_requirements.txt
RUN chmod +x /usr/local/bin/start_services.sh

# Set environment variables optimized for RTX 4090
ENV PATH="${VENV_PATH}/bin:${PATH}" \
    PYTHONUNBUFFERED=1 \
    # RTX 4090 optimizations
    TORCH_INDUCTOR_FORCE_DISABLE_FP8="1" \
    CUDA_VISIBLE_DEVICES="0" \
    PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512" \
    # ComfyUI specific settings
    COMFY_DIR="/workspace/ComfyUI" \
    COMFY_LAUNCH_ARGS="--listen 0.0.0.0 --port 8188 --disable-auto-launch --preview-method auto"

# Expose ComfyUI and API ports
EXPOSE 8188 8189

# Health check to ensure ComfyUI is running
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8188/ || exit 1

# Use tini as the entrypoint to properly manage processes
ENTRYPOINT ["/usr/bin/tini", "-s", "--", "/usr/local/bin/start_services.sh"]
